{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI Engineer Bootcamp by IYKRA\n",
        "\n",
        "Technical test is made by Ari Sulistiyo Prabowo\n",
        "\n",
        "1. Step 1: Data Ingestion\n",
        "2. Step 2: Data Preprocessing\n",
        "3. Step 3: GAN Training with TensorFlow\n",
        "4. Step 4: Model Deployment using Vertex AI"
      ],
      "metadata": {
        "id": "kdPoQHyXIhvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Data Ingestion\n"
      ],
      "metadata": {
        "id": "C8o7IZ0HDPMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Google Cloud Storage to store raw data from multiple sources"
      ],
      "metadata": {
        "id": "k40-G-a6JE5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "from google.cloud import bigquery\n",
        "\n",
        "def upload_to_gcs(bucket_name, file_name, data):\n",
        "    client = storage.Client()\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "    blob.upload_from_string(data)\n",
        "    print(f\"File {file_name} uploaded to {bucket_name}\")"
      ],
      "metadata": {
        "id": "Os3d2kXKDQSz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Ingestion"
      ],
      "metadata": {
        "id": "-jUH2UDVJBlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ecommerce_data = \"ecommerce_sales.csv\"\n",
        "social_media_data = \"social_trends.json\"\n",
        "bucket_name = \"fashion-data-lake\"\n",
        "upload_to_gcs(bucket_name, ecommerce_data, ecommerce_data)\n",
        "upload_to_gcs(bucket_name, social_media_data, social_media_data)"
      ],
      "metadata": {
        "id": "4E4lbpsfDTYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Data Preprocessing"
      ],
      "metadata": {
        "id": "uLXQqk8pDYE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using BigQuery to preprocess structured data, clean missing values, and normalize"
      ],
      "metadata": {
        "id": "dAiA6WYYJJEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data():\n",
        "    client = bigquery.Client()\n",
        "    query = \"\"\"\n",
        "        SELECT\n",
        "            product_id,\n",
        "            sales,\n",
        "            IFNULL(social_trends_score, 0) as trend_score,\n",
        "            (sales - MIN(sales) OVER()) / (MAX(sales) OVER() - MIN(sales) OVER()) as normalized_sales\n",
        "        FROM\n",
        "            `project.dataset.sales_data`\n",
        "        WHERE\n",
        "            sales IS NOT NULL\n",
        "    \"\"\"\n",
        "    job = client.query(query)\n",
        "    result = job.result()  # Waits for query to finish\n",
        "    return result\n",
        "\n",
        "processed_data = preprocess_data()"
      ],
      "metadata": {
        "id": "-LmLTA6BDU-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: GAN Training with Tensorflow"
      ],
      "metadata": {
        "id": "rENy9y5ZJwE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# GAN Generator Model\n",
        "def build_generator():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(128, input_dim=100))  # Noise projection into 128 units\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(256))  # Transforming to 256 units\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(512))  # Transforming to 512 units\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.Dense(28 * 28 * 1, activation='tanh'))  # Fashion design as 28x28 image\n",
        "    return model\n",
        "\n",
        "# GAN Discriminator Model\n",
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(512, input_shape=(28 * 28 * 1,))) #please fill in the empty value\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dense(256))\n",
        "    model.add(layers.LeakyReLU(0.2))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Compile GAN\n",
        "def compile_gan(generator, discriminator):\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    discriminator.trainable = False\n",
        "    gan_input = tf.keras.Input(shape=(100,))\n",
        "    gan_output = discriminator(generator(gan_input))\n",
        "    gan = tf.keras.Model(gan_input, gan_output)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return gan\n",
        "\n",
        "# Train GAN\n",
        "def train_gan(generator, discriminator, gan, epochs=1000, batch_size=128):\n",
        "    for epoch in range(epochs):\n",
        "        noise = tf.random.normal([batch_size, 100])\n",
        "        generated_images = generator.predict(noise)\n",
        "        real_images = processed_data.sample(batch_size)  # Assuming processed_data is available\n",
        "        labels_real = tf.ones((batch_size, 1))  # Real labels\n",
        "        labels_fake = tf.zeros((batch_size, 1))  # Fake labels\n",
        "\n",
        "        d_loss_real = discriminator.train_on_batch(real_images, labels_real)\n",
        "        d_loss_fake = discriminator.train_on_batch(generated_images, labels_fake)\n",
        "\n",
        "        noise = tf.random.normal([batch_size, 100])\n",
        "        g_loss = gan.train_on_batch(noise, tf.ones((batch_size, 1)))  # Train GAN to generate fake images as real\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, D Loss Real: {d_loss_real}, D Loss Fake: {d_loss_fake}, G Loss: {g_loss}\")\n"
      ],
      "metadata": {
        "id": "tio5qpBdDZ-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build and train the GAN"
      ],
      "metadata": {
        "id": "-nwJ2Y1oZtOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "gan = compile_gan(generator, discriminator)\n",
        "train_gan(generator, discriminator, gan)"
      ],
      "metadata": {
        "id": "fLJSo-2MZusO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Model Deployment using Vertex AI"
      ],
      "metadata": {
        "id": "wSO1-L_VZ0bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "def deploy_model_to_vertex_ai(model_artifact, endpoint_name):\n",
        "    aiplatform.init()\n",
        "    model = aiplatform.Model.upload(display_name=\"fashion-gan-model\", artifact_uri=model_artifact)\n",
        "    endpoint = model.deploy(machine_type=\"n1-standard-4\", endpoint_name=endpoint_name)\n",
        "    return endpoint\n",
        "\n",
        "endpoint = deploy_model_to_vertex_ai(\"gs://fashion-model-bucket/gan_model\", \"fashion-gan-endpoint\")\n",
        "\n",
        "# Step 5: Real-Time Inference\n",
        "import numpy as np\n",
        "\n",
        "def generate_fashion_design():\n",
        "    noise = np.random.normal(0, 1, (1, 100))\n",
        "    design = generator.predict(noise)\n",
        "    print(f\"Generated fashion design: {design}\")\n",
        "    return design\n",
        "\n",
        "generate_fashion_design()"
      ],
      "metadata": {
        "id": "qr2YI8hYZ2VR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}