apiVersion: apps/v1
kind: Deployment
metadata:
  name: gan-inference-deployment
spec:
  replicas: 2  # Initial number of replicas
  selector:
    matchLabels:
      app: gan-inference
  template:
    metadata:
      labels:
        app: gan-inference
    spec:
      containers:
      - name: gan-inference-container
        image: gcr.io/your-project-id/gan-inference:latest  # Replace with your image
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1024Mi"
            cpu: "1000m"
---
apiVersion: v1
kind: Service
metadata:
  name: gan-inference-service
spec:
  selector:
    app: gan-inference
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
  type: LoadBalancer  # Expose the service via an external load balancer
---
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: gan-inference-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: gan-inference-deployment
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 75  # Scale up when CPU usage exceeds 75%
